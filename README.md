# The Rise and Potential of Large Language Model Based Agents: A Survey
ðŸ”¥ **Must-read papers for LLM-based agents.**

## ðŸ”” News
- [2023/09/14] We will release our survey paper: Large Language Model Based Agents: The Past, Present, and Potential . 
  - You can also download it here:
- [2023/09/14] We create this repository to introduce a survey of LLM-based agents and list some must-read papers.

<div align=center><img src="./assets/figure1.jpg" width="80%" /></div>


## ðŸŒŸ Introduction

For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing human level, with AI agents considered as a promising vehicle of this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. 

Due to the versatile and remarkable capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many research efforts have leveraged LLMs as the foundation to build AI agents and have achieved significant progress.

In this repository, we provide a systematic and comprehensive survey on LLM-based agents, and list some must-read papers. **We greatly appreciate any contributions via PRs, issues, emails, or other methods.**

Specifically, we start by the general conceptual framework for LLM-based agents: comprising three main components: brain, perception, and action, and the framework can be tailored to suit different applications. 
Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. 
Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge when they form societies, and the insights they offer for human society.
Finally, we discuss a range of key topics and open problems within the field.







## The Birth of An Agent: Construction of LLM-based Agents
<div align=center><img src="./assets/figure2.jpg" width="60%" /></div>

### Brain: Primarily Composed of An LLM

### Perception: Multimodal Inputs for LLM-based Agents

### Action: Expand Action Space of LLM-based Agents

#### Tool Using

#### Embodied Action
- [2023/07] Interactive language: Talking to robots in real time *Corey Lynch et al. IEEE(RAL)* [[paper](https://arxiv.org/pdf/2210.06407.pdf)]
- [2023/05] Voyager: An open-ended embodied agent with large language models *Guanzhi Wang et al. Arxiv.* [[paper](https://arxiv.org/pdf/2305.16291.pdf)]
- [2023/05] AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments *Sudipta Paul et al. NeurIPS.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/28f699175783a2c828ae74d53dd3da20-Paper-Conference.pdf)]
- [2023/05] EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought *Yao Mu et al. Arxiv* [[paper](https://arxiv.org/pdf/2305.15021.pdf)] [[code](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch)]
- [2023/05] NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models *Gengze Zhou et al. Arxiv* [[paper](https://arxiv.org/pdf/2305.16986.pdf)]
- [2023/05] AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation *Chuhao Jin et al. Arxiv* [[paper](https://arxiv.org/pdf/2305.18898.pdf)]
- [2023/03] PaLM-E: An Embodied Multimodal Language Model *Danny Driess et al. Arxiv.* [[paper](https://arxiv.org/pdf/2303.03378.pdf)] 
- [2023/03] Reflexion: Language Agents with Verbal Reinforcement Learning *Noah Shinn et al. Arxiv* [[paper](https://arxiv.org/pdf/2303.11366.pdf)] [[code](https://github.com/noahshinn024/reflexion)]
- [2023/02] Collaborating with language models for embodied reasoning *Ishita Dasgupta et al. Arxiv.* [[paper](https://arxiv.org/pdf/2302.00763.pdf)]
- [2023/02] Code as Policies: Language Model Programs for Embodied Control *Jacky Liang et al. IEEE(ICRA).* [[paper](https://arxiv.org/pdf/2209.07753.pdf)] 
- [2022/10] ReAct: Synergizing Reasoning and Acting in Language Models *Shunyu Yao et al. Arxiv* [[paper](https://arxiv.org/pdf/2210.03629.pdf)] [[code](https://github.com/ysymyth/ReAct)]
- [2022/10] Instruction-Following Agents with Multimodal Transformer *Hao Liu et al. CVPR* [[paper](https://arxiv.org/pdf/2210.13431.pdf)] [[code](https://github.com/lhao499/instructrl)]
- [2022/07] Inner Monologue: Embodied Reasoning through Planning with Language Models  *Wenlong Huang et al. Arxiv.* [[paper](https://arxiv.org/pdf/2207.05608.pdf)]
- [2022/07] LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action *Dhruv Shahet al. CoRL* [[paper](https://proceedings.mlr.press/v205/shah23b/shah23b.pdf)] [[code](https://github.com/blazejosinski/lm_nav)]
- [2022/04] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances *Michael Ahn et al. Arxiv.* [[paper](https://arxiv.org/pdf/2204.01691.pdf)] 
- [2022/01] A Survey of Embodied AI: From Simulators to Research Tasks *Jiafei Duan et al. IEEE(TETCI).* [[paper](https://arxiv.org/pdf/2103.04918.pdf)] 
- [2022/01] Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents *Wenlong Huang et al. Arxiv.* [[paper](https://arxiv.org/pdf/2201.07207v2.pdf)] [[code](https://github.com/huangwl18/language-planner)] 
- [2020/04] Experience Grounds Language *Yonatan Bisk et al. EMNLP* [[paper](https://arxiv.org/pdf/2004.10151.pdf)]
- [2019/03] Review of Deep Reinforcement Learning for Robot Manipulation *Hai Nguyen et al. IEEE(IRC).* [[paper](https://www.researchgate.net/profile/Hai-Nguyen-128/publication/355980729_Review_of_Deep_Reinforcement_Learning_for_Robot_Manipulation/links/6187ef153068c54fa5bb977e/Review-of-Deep-Reinforcement-Learning-for-Robot-Manipulation.pdf)]
- [2005/01] The Development of Embodied Cognition: Six Lessons from Babies *Linda Smith et al. Artificial Life.* [[paper](https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf)]



## Agents in Practice: Applications of LLM-based Agents

<div align=center><img src="./assets/figure7.jpg" width="60%" /></div>

### General Ability of Single Agent

### Coordinating Potential of Multiple Agents


### Interactive Engagement between Human and Agent


## Agent Society: From Individuality to Sociality
<div align=center><img src="./assets/figure12.jpg" width="60%" /></div>

### Behavior and Personality of LLM-based Agent

### Environment for Agent Society


### Society Simulation with LLM-based Agents

1. [2023/04] Generative Agents: Interactive Simulacra of Human Behavior. *Joon Sung Park (Stanford) et al. arXiv.* [[paper](https://arxiv.org/abs/2304.03442)] [[code](https://github.com/joonspk-research/generative_agents)]
   - Agent simulation with LLM-based agents. Some social phenomena are observed.
2. [2023/08] AgentSims: An Open-Source Sandbox for Large Language Model Evaluation. *Jiaju Lin (PTA Studio) et al. arXiv.*[[paper](https://arxiv.org/abs/2304.03442)] [[code](https://github.com/joonspk-research/generative_agents)]
   - A simulated environment for evaluation tasks.


## Project Maintainers & Contributors



## Contact
